#!/bin/bash

CONTAINER_NAME=$1
CONFIG_MOUNT=$2
APPLICATION=$3
RESOURCES=$4
DATASOURCE=$5

LOCATION="us-east4-a"
CLUSTER=qitransientcluster
REG_SERVER=gcr.io/${RESOURCES}

# Given a source directory, copies its contents to a GCP bucket
#
# ARGUMENTS:
#   _SOURCE_DIR  - The local machine source to copy
#   _BUCKET_NAME - The GCP Data Bucket to upload data
#   _AUTH_CONFIG - The pre-authenticated config container
#
function CopyData()
{
    _SOURCE_DIR=$1
    _BUCKET_NAME=$2
    _AUTH_CONFIG=$3

    docker run \
        --rm \
        --mount type=bind,source="${_SOURCE_DIR}",target=/mnt/data \
        --volumes-from ${_AUTH_CONFIG} \
        google/cloud-sdk \
        gsutil -m \
            cp -r /mnt/data/* gs://${_BUCKET_NAME}
}

function GetPlatformContainer()
{
    # Use the Prebuilt gcloud sdk container provided by Google.
    docker pull google/cloud-sdk
}

# Login with the container. This currently requires user interaction
# at the browser to complete. The below command creates a volume
# at the ${CONFIG_MOUNT} location in order to persist kubernetes
# login credentials by setting the KUBECONFIG environment variable.
#
function Login()
{
    local _container_name=$1
    local _config_mount=$2

    docker run \
        -it \
        --name ${_container_name} \
        --label qi_container_type=login-config \
        -e KUBECONFIG=${_config_mount}/kube_config \
        --mount type=volume,target=${_config_mount} \
        --mount type=bind,source="$(dirname `GetAbsPath ${BASH_SOURCE[0]}`)/scripts",target=/mnt/startup \
        google/cloud-sdk \
        gcloud auth login
}

# Common docker command component for gcloud and kubectl commands inside
# of the google/cloud-sdk container.
#
PREFIX="docker run \
    --rm \
    -e KUBECONFIG=${CONFIG_MOUNT}/kube_config \
    --volumes-from ${CONTAINER_NAME} \
    google/cloud-sdk"

GCLOUD="$PREFIX gcloud"
KUBECTL="$PREFIX kubectl"
GSUTIL="$PREFIX gsutil"

function Provision()
{
    local _resources=$1
    local _location=$2
    local _datasource=$3

    local _bucket_name=$(GetId "$_datasource")
    local _bucket_name="${_bucket_name}-qi-data-upload"

    # Create a hosting project for transient resources.
    $GCLOUD projects create $_resources

    $GCLOUD config set project $_resources
    $GCLOUD config set compute/zone $_location

    # GCP requires setting up billing on a project before certain services
    # can be used. GCP does not automatically create a link to billing
    # as the billing structure of GCP allows for many billing accounts within
    # an organization.
    echo
    echo CHOOSE A BILLING ACCOUNT TO LINK WITH "\"${_resources}\""

    # Create a menu for billing account options.
    let option_id=0
    accounts=()
    while IFS= read -r line; do
        let option_id++

        # Provide an options menu to select a billing account.
        echo "[${option_id}] ${line}"
        account_id="${line%% *}"
        accounts+=("$account_id")

    # Lists all of the available billing accounts for the current user
    # formatted to list the account number and the display name.
    #
    done < <(docker run \
        --rm \
        -e KUBECONFIG=${CONFIG_MOUNT}/kube_config \
        --volumes-from ${CONTAINER_NAME} \
        google/cloud-sdk \
        gcloud beta billing accounts list \
        --format "table[no-heading](name,displayName)")

    echo
    read -n 1 -p "> " CHOICE
    echo

    let CHOICE--

    $GCLOUD alpha billing projects link $_resources --billing-account ${accounts[CHOICE]}

    # Enable the use of the container and the container registry APIs for project
    $GCLOUD services enable containerregistry.googleapis.com
    $GCLOUD services enable container.googleapis.com

    # Provision dat bucket
    $GSUTIL mb -c regional -l ${_location: 0:-2} gs://$_bucket_name
}

function PrepareDocker()
{
    # Login to gcr.io
    $GCLOUD auth print-access-token | docker login \
        -u oauth2accesstoken \
        --password-stdin https://gcr.io
}

PrepareDocker

REMOTE_IMAGE=$(PushToRemote ${APPLICATION} ${REG_SERVER})

function Deploy()
{

    local _address="$1"
    local _compute_tag="$2"
    local _datasource="$3"

    local _bucket_name=$(GetId "$_datasource")
    local _bucket_name="${_bucket_name}-qi-data-upload"

    $GCLOUD compute addresses create ${_address} \
        --region us-east4

    BackgroundTask CopyData $_datasource $_bucket_name $CONTAINER_NAME

    # Automatically stop this resource (GPU cost is high).
    $GCLOUD compute instances create \
        $APPLICATION                               \
        --zone ${LOCATION}                         \
        --accelerator type=nvidia-tesla-p4,count=1 \
        --custom-cpu 8                             \
        --custom-memory 32GB                       \
        --maintenance-policy TERMINATE             \
        --restart-on-failure                       \
        --image-family ubuntu-1804-lts             \
        --image-project ubuntu-os-cloud            \
        --boot-disk-size 200GB                     \
        --address ${_address}                      \
        --tags ${_compute_tag}                     \
        --metadata \
        __qi_container_name="$REMOTE_IMAGE",__qi_app_bucket=$_bucket_name \
        --metadata-from-file startup-script=/mnt/startup/ubuntu-startup.sh
 
    $GCLOUD compute firewall-rules create allow-cn-server \
        --allow tcp:8888 --target-tags ${_compute_tag}

    # TODO:
    # Use Cloud Function to create a callback on startup-script
    # completion?

    JUPYTER_SERVER=$($GCLOUD \
        compute addresses describe \
        ${_address} \
        --region us-east4 \
        --format "table[no-heading](address)")
}

function ConnectToServer()
{
    :
}
